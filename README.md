# Theoretical Advantages of Bilinear Similarities in Dense Information Retrieval

This repository contains the complete implementation and empirical validation for the paper **"On the Theoretical Advantages of Bilinear Similarities in Dense Information Retrieval."**

## ğŸ“– Paper Overview

We present a comprehensive theoretical analysis establishing that bilinear similarity functions (`s(q,d) = q^T W d`) offer fundamental expressiveness advantages over standard dot-product and weighted dot-product similarities when query and document embeddings are **fixed**.

### Key Theoretical Contributions

1. **Enhanced Expressiveness (Theorem 2.1):** We prove that bilinear similarities can capture strictly more ranking patterns than dot-product similarities under fixed embeddings.

2. **Separation via Structured Task (Theorem 3.1):** We introduce the "Structured Agreement Ranking Task" where simple rank-2 bilinear models achieve perfect performance, while all weighted dot-product models are proven to fail universally.

3. **Low-Rank Approximation Bounds (Theorem 4.1):** We derive tight pointwise error bounds for low-rank approximations of bilinear matrices, showing that approximation quality is controlled by neglected singular values.

## ğŸ—ï¸ Repository Structure

```
bilinear-proj-theory/
â”œâ”€â”€ docs/                        
â”‚   â”œâ”€â”€ Experiment1.md                  # Synthetic experiments (direct theoretical validation)                   
â”‚   â”œâ”€â”€ Experiment2.md                   # MS MARCO experiments (real-world validation)
    â”œâ”€â”€ Experiment3.md
â”œâ”€â”€ README.md                           # This file - general overview
â”œâ”€â”€ requirements.txt                    # Common dependencies
â”‚
â”œâ”€â”€ experiment1/                        # Synthetic Agreement Task Experiments
â”‚   â”œâ”€â”€ config.py                       # Experiment 1 configuration
â”‚   â”œâ”€â”€ experiment1.py                  # Main script for synthetic experiments
â”‚   â”œâ”€â”€ models.py                       # Bilinear and WDP models
â”‚   â”œâ”€â”€ synthetic_data_gen.py          # Data generation utilities
â”‚   â””â”€â”€ saved_results_exp1/            # Results directory
â”‚
â”œâ”€â”€ experiment2/                        # MS MARCO Real-World Experiments
â”‚   â”œâ”€â”€ config.py                       # Experiment 2 configuration
â”‚   â”œâ”€â”€ main_train.py                   # Main training script
â”‚   â”œâ”€â”€ models.py                       # Model definitions (WDP, LRB, FRB)
â”‚   â”œâ”€â”€ data_loader.py                  # Dataset utilities
â”‚   â”œâ”€â”€ evaluate.py                     # Evaluation functions
â”‚   â”œâ”€â”€ preprocess_embeddings.py        # Embedding generation
â”‚   â”œâ”€â”€ embeddings/                     # Pre-computed embeddings
â”‚   â”œâ”€â”€ data/                          # MS MARCO data
â”‚   â”œâ”€â”€ saved_models/                   # Trained models and results
â”‚   â””â”€â”€ ms_marco_eval/                 # MS MARCO evaluation scripts
â”‚ 
â”œâ”€â”€ experiment3/                        # Low-Rank Approximation Analysis
â”‚   â”œâ”€â”€ config.py                       # Experiment 3 configuration
â”‚   â”œâ”€â”€ main_experiment3.py             # SVD and approximation analysis
â”‚   â”œâ”€â”€ init.py                    # Package structure
â”‚   â””â”€â”€ saved_results_exp3/            # Analysis results and plots
â”‚ 
â””â”€â”€ analysis/                          # Analysis and visualization scripts
    â”œâ”€â”€ plot_results.py                # Visualization utilities
    â””â”€â”€ comparative_analysis.py        # Cross-experiment analysis
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install common dependencies
pip install -r requirements.txt

# Or create a conda environment
conda create -n bilinear-theory python=3.9
conda activate bilinear-theory
pip install -r requirements.txt
```

### Choose Your Experiment

#### Option 1: Quick Theoretical Validation (Recommended for understanding)

Run the synthetic experiments to directly validate the theoretical claims:

```bash
cd experiment1/
python experiment1.py
```

**Time Required:** ~5-10 minutes  
**What it does:** Validates Theorems 3.1.i and 3.1.ii using the Structured Agreement Task

ğŸ‘‰ **[Detailed Instructions for Experiment 1](docs/Experiment1.md)**

#### Option 2: Real-World Performance Validation

Run the MS MARCO experiments to see practical implications:

```bash
cd experiment2/
# First, prepare data and embeddings (takes time)
python preprocess_embeddings.py
# Then run training
python main_train.py
```

**Time Required:** ~8-12 hours (depending on hardware)  
**What it does:** Validates theoretical advantages on a real-world IR benchmark

ğŸ‘‰ **[Detailed Instructions for Experiment 2](docs/Experiment2.md)**

#### Option 3: Low-Rank Approximation Analysis

Analyze the behavior of low-rank approximations:

```bash
cd experiment3/
python -m experiment3.main_experiment3
```
**Time Required:** ~30-60 minutes
**What it does:** Validates Theorem 4.1 by analyzing singular value decomposition of trained bilinear models

ğŸ‘‰ **[Detailed Instructions for Experiment 3](docs/Experiment3.md)**

## ğŸ”¬ Experiment Overview

### Experiment 1: Synthetic Agreement Task

- **Purpose:** Direct validation of Theorem 3.1
- **Data:** Synthetic hypercube embeddings in {-1, +1}^n
- **Key Result:** Bilinear models achieve 100% success, WDP models fail universally
- **Runtime:** Minutes
- **Insight:** Shows why bilinear models excel at feature interaction tasks

### Experiment 2: MS MARCO Passage Ranking

- **Purpose:** Real-world validation of all theoretical claims
- **Data:** MS MARCO V1 with fixed SBERT embeddings
- **Key Result:** Low-rank bilinear models significantly outperform dot-product baselines
- **Runtime:** Hours
- **Insight:** Demonstrates practical value of theoretical advantages

### Experiment 3: Low-Rank Approximation Analysis

- **Purpose:** Validate Theorem 4.1 and analyze efficiency/performance trade-offs
- **Data:** Pre-trained bilinear models from Experiment 2
- **Key Result:** Most performance achieved with much lower rank (e.g., rank 64 vs. full rank)
- **Runtime:** 30-60 minutes
- **Insight:** Provides practical guidance for efficient bilinear model deployment

## ğŸ“Š Key Findings

### Theoretical Validation

1. **Theorem 3.1.i âœ“**: Rank-2 bilinear models achieve 100% success on agreement task
2. **Theorem 3.1.ii âœ“**: No single WDP model can solve all agreement patterns
3. **Theorem 4.1 âœ“**: Low-rank approximation error follows predicted bounds

### Practical Impact

1. **MS MARCO Results**: Low-rank bilinear models (rank 64) achieve significant improvements:
   - +3-5% MRR@10 over dot-product baseline
   - +1-2% MRR@10 over weighted dot-product
   - Consistent gains across multiple metrics

2. **Efficiency**: Low-rank approximations capture most benefits:
   - Rank 64 achieves 90%+ of full-rank performance
   - 10x fewer parameters than full bilinear matrix
   - Practical for large-scale deployment

## ğŸ”§ Advanced Usage

### Custom Experiments

Both experiment directories contain modular code that can be easily customized:

```python
# Experiment 1: Test different dimensions or patterns
# In experiment1/config.py
DIM_N = 20  # Increase problem difficulty

# Experiment 2: Try different embedding models or ranks
# In experiment2/config.py
SBERT_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
```

### Analysis and Visualization

```bash
# Generate plots and analysis
cd analysis/
python plot_results.py
python comparative_analysis.py
```

## ğŸ“ Citation

If you use this code or build upon this work, please cite our paper:

```bibtex
@article{author2024bilinear,
  title={On the Theoretical Advantages of Bilinear Similarities in Dense Information Retrieval},
  author={Author Name},
  journal={Conference/Journal Name},
  year={2024}
}
```

## ğŸ¤ Contributing

We welcome contributions! Please:

1. Review both experiment READMEs to understand the codebase
2. Create an issue to discuss proposed changes
3. Submit a pull request with clear documentation

## ğŸ“® Contact

For questions about the implementation or paper, please:
- Open an issue on this repository
- Contact [author email]

## ğŸ“š Additional Resources

- **Paper**: [Link to paper when available]
- **Experiment 1 Details**: [Experiment1.md](docs/Experiment1.md)
- **Experiment 2 Details**: [Experiment2.md](docs/Experiment2.md)
- **Experiment 3 Details**: [Experiment3.md](docs/Experiment3.md)
- **Supplementary Materials**: [Link if available]

---

---

**Note**: Start with Experiment 1 for a quick understanding of the core theoretical insights, then proceed to Experiment 2 
for real-world validation. After training models in Experiment 2, run Experiment 3 to analyze low-rank approximation properties.